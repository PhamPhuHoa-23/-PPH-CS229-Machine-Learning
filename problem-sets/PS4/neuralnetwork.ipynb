{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9548241,"sourceType":"datasetVersion","datasetId":5817473}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\n\nMAX_POOL_SIZE = 5\nCONVOLUTION_SIZE = 4\nCONVOLUTION_FILTERS = 2","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.021899Z","start_time":"2024-10-04T15:46:12.039022Z"},"execution":{"iopub.status.busy":"2024-10-04T15:54:55.632953Z","iopub.execute_input":"2024-10-04T15:54:55.633518Z","iopub.status.idle":"2024-10-04T15:54:55.665289Z","shell.execute_reply.started":"2024-10-04T15:54:55.633455Z","shell.execute_reply":"2024-10-04T15:54:55.664165Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def forward_softmax(x):\n    \"\"\"\n    Compute softmax function for a single example.\n    The shape of the input is of size # num classes.\n\n    Important Note: You must be careful to avoid overflow for this function. Functions\n    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n    You will know that your function is overflow resistent when it can handle input like:\n    np.array([[10000, 10010, 10]]) without issues.\n\n        x: A 1d numpy float array of shape number_of_classes\n\n    Returns:\n        A 1d numpy float array containing the softmax results of shape  number_of_classes\n    \"\"\"\n    x = x - np.max(x,axis=0)\n    exp = np.exp(x)\n    s = exp / np.sum(exp,axis=0)\n    return s","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.044157Z","start_time":"2024-10-04T15:46:16.029880Z"},"execution":{"iopub.status.busy":"2024-10-04T15:54:58.319624Z","iopub.execute_input":"2024-10-04T15:54:58.320461Z","iopub.status.idle":"2024-10-04T15:54:58.328309Z","shell.execute_reply.started":"2024-10-04T15:54:58.320398Z","shell.execute_reply":"2024-10-04T15:54:58.326491Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Finding backward softmax equation\n\n\\begin{align*}\n    \\displaystyle\n    \\frac{\\partial l}{\\partial x_{\\gamma}} & = \\frac{\\partial l}{\\partial \\hat{y}_{\\gamma}} \\frac{\\partial \\hat{y}_{\\gamma}}{\\partial x_{\\gamma}} \\\\\n    & = \\frac{\\partial l}{\\partial \\hat{y}_{\\gamma}} \\frac{\\partial}{\\partial x_{\\gamma}} \\bigg( \\frac{e^{x_{\\gamma}}}{\\sum_{i = 1}^K e^{x_{i}}} \\bigg ) \\\\\n    & = \\frac{\\partial l}{\\partial \\hat{y}_{\\gamma}} \\frac{e^{x_{\\gamma}} \\big( \\sum_{i = 1}^K e^{x_{i}} \\big) - e^{x_{\\gamma}} e^{x_{\\gamma}}}{(\\sum_{i = 1}^K e^{x_i})^2} \\\\\n    & = \\frac{\\partial l}{\\partial \\hat{y}_{\\gamma}} (\\hat{y}_{\\gamma} - {\\hat{y}_{\\gamma}}^2)\n\\end{align*}","metadata":{}},{"cell_type":"code","source":"def backward_softmax(x, grad_outputs):\n    \"\"\"\n    Compute the gradient of the loss with respect to x.\n\n    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n\n    Args:\n        x: A 1d numpy float array of shape number_of_classes\n        grad_outputs: A 1d numpy float array of shape number_of_classes\n\n    Returns:\n        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n    \"\"\"\n    \n    # *** START CODE HERE ***\n    forward_values = forward_softmax(x)\n#     return grad_outputs * (1 - forward_values) * forward_values\n    return grad_outputs @ (np.diag(forward_values) - np.outer(forward_values, forward_values))\n    # *** END CODE HERE ***\n","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.055863Z","start_time":"2024-10-04T15:46:16.048148Z"},"execution":{"iopub.status.busy":"2024-10-04T17:33:19.157690Z","iopub.execute_input":"2024-10-04T17:33:19.158101Z","iopub.status.idle":"2024-10-04T17:33:19.164531Z","shell.execute_reply.started":"2024-10-04T17:33:19.158061Z","shell.execute_reply":"2024-10-04T17:33:19.163381Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def forward_relu(x):\n    \"\"\"\n    Compute the relu function for the input x.\n\n    Args:\n        x: A numpy float array\n\n    Returns:\n        A numpy float array containing the relu results\n    \"\"\"\n    x[x<=0] = 0\n\n    return x","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.067365Z","start_time":"2024-10-04T15:46:16.059856Z"},"execution":{"iopub.status.busy":"2024-10-04T15:55:04.587859Z","iopub.execute_input":"2024-10-04T15:55:04.588285Z","iopub.status.idle":"2024-10-04T15:55:04.593905Z","shell.execute_reply.started":"2024-10-04T15:55:04.588245Z","shell.execute_reply":"2024-10-04T15:55:04.592709Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Finding backward ReLU equation\n\nNotation: $y$ = ReLU($x$) \n\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x} & = \\Bigg\\{ \\begin{matrix}\n        1 &  \\text{ x > 0 } \\\\\n        0 &  \\text{ otherwise}\n    \\end{matrix}\n\\end{align*}\n\n\\begin{align*}\n    \\displaystyle\n    \\frac{\\partial l}{\\partial x_{}} & = \\frac{\\partial l}{\\partial {y}_{}} \\frac{\\partial {y}_{}}{\\partial x_{}} \\\\\n\\end{align*}","metadata":{}},{"cell_type":"code","source":"def backward_relu(x, grad_outputs):\n    \"\"\"\n    Compute the gradient of the loss with respect to x\n\n    Args:\n        x: A numpy array of arbitrary shape containing the input.\n        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n            to the output of relu\n\n    Returns:\n        A numpy array of the same shape as x containing the gradients with respect to x.\n    \"\"\"\n\n    # *** START CODE HERE ***\n    grad_outputs[x <= 0] = 0\n    return grad_outputs\n    # *** END CODE HERE ***","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.101376Z","start_time":"2024-10-04T15:46:16.070874Z"},"execution":{"iopub.status.busy":"2024-10-04T15:55:06.408872Z","iopub.execute_input":"2024-10-04T15:55:06.409320Z","iopub.status.idle":"2024-10-04T15:55:06.415254Z","shell.execute_reply.started":"2024-10-04T15:55:06.409270Z","shell.execute_reply":"2024-10-04T15:55:06.414118Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_initial_params():\n    \"\"\"\n    Compute the initial parameters for the neural network.\n\n    This function should return a dictionary mapping parameter names to numpy arrays containing\n    the initial values for those parameters.\n\n    There should be four parameters for this model:\n    W1 is the weight matrix for the convolutional layer\n    b1 is the bias vector for the convolutional layer\n    W2 is the weight matrix for the output layers\n    b2 is the bias vector for the output layer\n\n    Weight matrices should be initialized with values drawn from a random normal distribution.\n    The mean of that distribution should be 0.\n    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n    feed into an output for that layer.\n\n    Bias vectors should be initialized with zero.\n    \n    \n    Returns:\n        A dict mapping parameter names to numpy arrays\n    \"\"\"\n    size_after_taking_convolution = 28 - CONVOLUTION_SIZE + 1\n    size_after_taking_maxpooling = size_after_taking_convolution // MAX_POOL_SIZE\n\n    number_of_flatten = size_after_taking_maxpooling * size_after_taking_maxpooling * CONVOLUTION_FILTERS\n\n    return {\n        'W1': np.random.normal(size=(CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1 / math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n        'b1': np.zeros((CONVOLUTION_FILTERS,)),\n        \"W2\": np.random.normal(size=(number_of_flatten, 10), scale=1 / number_of_flatten),\n        'b2': np.zeros(10)\n    }","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.173455Z","start_time":"2024-10-04T15:46:16.160334Z"},"execution":{"iopub.status.busy":"2024-10-04T16:37:34.669758Z","iopub.execute_input":"2024-10-04T16:37:34.670188Z","iopub.status.idle":"2024-10-04T16:37:34.679505Z","shell.execute_reply.started":"2024-10-04T16:37:34.670148Z","shell.execute_reply":"2024-10-04T16:37:34.678300Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def forward_convolution(conv_W, conv_b, data):\n    \"\"\"\n    Compute the output from a convolutional layer given the weights and data.\n\n    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height ) <= Weights\n    conv_b is of the shape (# output channels) <- Bias\n\n    data is of the shape (# input channels, width, height) <- Input\n\n    The output should be the result of a convolution and should be of the size:\n        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n\n    Returns:\n        The output of the convolution as a numpy array\n    \"\"\"\n\n    output_channels, input_channels, conv_width, conv_height = conv_W.shape\n    _, input_width, input_height = data.shape\n    output = np.zeros(shape=(output_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n\n    for x in range(input_width - conv_width + 1):\n        for y in range(input_height - conv_height + 1):\n            for output_channel in range(output_channels):\n                output[output_channel, x, y] = np.sum(\n                    np.multiply(\n                        data[:, x:(x+conv_width), y:(y+conv_height)],\n                        conv_W[output_channel, :, :, :]\n                    )\n                ) + conv_b[output_channel]\n    return output","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.209423Z","start_time":"2024-10-04T15:46:16.176442Z"},"execution":{"iopub.status.busy":"2024-10-04T16:33:42.070034Z","iopub.execute_input":"2024-10-04T16:33:42.070926Z","iopub.status.idle":"2024-10-04T16:33:42.080924Z","shell.execute_reply.started":"2024-10-04T16:33:42.070873Z","shell.execute_reply":"2024-10-04T16:33:42.079656Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Finding Backward Convolution\n\n$output[o\\_channel, x, y] = conv\\_b[o\\_channel] + \\sum_{d_i \\ d_j \\ i\\_channel} input[i\\_channel, x + d_i, y + d_j] * conv\\_W[o\\_channel, i\\_channel, d_i, d_j] $\n\n\\begin{align*}\n    \\frac{\\partial l}{\\partial b_{o\\_channel}} & = \\sum_{o\\_channel} \\sum_{x} \\sum_{y} \\frac{\\partial l}{\\partial o} \\frac{\\partial o}{\\partial b} \\\\\n    & = \\sum_{o\\_channel} \\sum_{x} \\sum_{y} \\frac{\\partial l}{\\partial o_{o\\_channel}}\n\\end{align*}\n\n\\begin{align*}\n    \\frac{\\partial l}{\\partial w_{o_c, i_c, d_i, d_j}} & = \\sum_{x, y} \\frac{\\partial l}{\\partial o_{o_c, x, y}} \\frac{\\partial o_{o_c, x, y}}{\\partial w_{o_c, i_c, d_i, d_i}} \\\\\n    & = \\sum_{x, y} \\frac{\\partial l}{\\partial o_{o_c, x, y}} i_{i_c, x + d_i, y + d_j} \\\\\n    % & = \\sum_{x, y, i_c} \\frac{\\partial l}{\\partial o_{o_c, x, y}} i_{i_c, x + d_i, y + d_j}\n\\end{align*}\n\n\\begin{align*}\n    \\frac{\\partial l}{\\partial i_{i\\_channel, x + d_i, y + d_j}} &= \\sum_{o\\_channel} \\frac{\\partial l}{\\partial o_{o\\_channel, i\\_channel, x, y}} \\frac{\\partial o_{o\\_channel, i\\_channel, x, y}}{\\partial i_{i\\_channel, x + d_i, y + d_j}} \\\\\n    &= \\sum_{o\\_channel} \\frac{\\partial l}{\\partial o_{o\\_channel, i\\_channel, x, y}} w_{o\\_channel, i\\_channel, d_i, d_j}\n\\end{align*}\n","metadata":{}},{"cell_type":"code","source":"def backward_convolution(conv_W, conv_b, data, output_grad):\n    \"\"\"\n    Compute the gradient of the loss with respect to the parameters of the convolution.\n\n    See forward_convolution for the sizes of the arguments.\n    output_grad is the gradient of the loss with respect to the output of the convolution.\n\n    Returns:\n        A tuple containing 3 gradients.\n        The first element is the gradient of the loss with respect to the convolution weights\n        The second element is the gradient of the loss with respect to the convolution bias\n        The third element is the gradient of the loss with respect to the input data\n    \"\"\"\n\n    # *** START CODE HERE ***\n    bias_grad = output_grad.sum(axis=(1, 2)) # <- Sum follow height and width\n\n    output_channels, _, conv_width, conv_height = conv_W.shape\n    input_channels, input_height, input_width = data.shape\n\n    weight_grad = np.zeros((conv_W.shape))\n    input_grad = np.zeros((data.shape))\n\n    for x in range(input_width - conv_width + 1):\n        for y in range(input_height - conv_height + 1):\n            for output_channel in range(output_channels):\n                weight_grad[output_channel, :, :, :] += output_grad[output_channel, x, y] * data[:, x:(x+conv_width), y:(y+conv_height)]\n                input_grad[:, x:(x + conv_width), y:(y + conv_height)] += output_grad[output_channel, x, y] * conv_W[output_channel, :, :, :]\n\n    return (weight_grad, bias_grad, input_grad)\n    # *** END CODE HERE ***","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.294155Z","start_time":"2024-10-04T15:46:16.220417Z"},"execution":{"iopub.status.busy":"2024-10-04T16:54:51.541349Z","iopub.execute_input":"2024-10-04T16:54:51.542530Z","iopub.status.idle":"2024-10-04T16:54:51.556609Z","shell.execute_reply.started":"2024-10-04T16:54:51.542475Z","shell.execute_reply":"2024-10-04T16:54:51.555368Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def forward_max_pool(data, pool_width, pool_height):\n    \"\"\"\n    Compute the output from a max pooling layer given the data and pool dimensions.\n\n    The stride length should be equal to the pool size\n\n    data is of the shape (# channels, width, height)\n\n    The output should be the result of the max pooling layer and should be of size:\n        (# channels, width // pool_width, height // pool_height)\n\n    Returns:\n        The result of the max pooling layer\n    \"\"\"\n    input_channels, input_width, input_height = data.shape\n\n    output = np.zeros(shape=(input_channels, input_width // pool_width, input_height // pool_height))\n\n    for x in range(0, input_width, pool_width):\n        for y in range(0, input_height, pool_height):\n            output[:, x // pool_width, y // pool_height] = np.max(\n                data[:, x:x+pool_width, y:y+pool_height], axis=(1, 2)\n            )\n\n    return output","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.310140Z","start_time":"2024-10-04T15:46:16.299151Z"},"execution":{"iopub.status.busy":"2024-10-04T16:35:35.473535Z","iopub.execute_input":"2024-10-04T16:35:35.473996Z","iopub.status.idle":"2024-10-04T16:35:35.482609Z","shell.execute_reply.started":"2024-10-04T16:35:35.473950Z","shell.execute_reply":"2024-10-04T16:35:35.481317Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def backward_max_pool(data, pool_width, pool_height, output_grad):\n    \"\"\"\n    Compute the gradient of the loss with respect to the data in the max pooling layer.\n\n    data is of the shape (# channels, width, height)\n    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n\n    output_grad is the gradient of the loss with respect to the output of the backward max\n    pool layer.\n\n    Returns:\n        The gradient of the loss with respect to the data (of same shape as data)\n    \"\"\"\n\n    # *** START CODE HERE ***\n    # *** END CODE HERE ***\n    output = np.zeros(shape=data.shape)\n    input_channels, input_width, input_height = data.shape\n\n    for input_channel in range(input_channels):\n        for x in range(0, input_width, pool_width):\n            for y in range(0, input_height, pool_height):\n                sub_data = data[input_channel, x:x+pool_width, y:y+pool_height]\n                output[input_channel, x:x+pool_width, y:y+pool_height][input_channel, np.unravel_index(sub_data.argmax(), sub_data.shape)] = output_grad[input_channel, x // pool_width, y // pool_height]\n    return output","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.451499Z","start_time":"2024-10-04T15:46:16.325056Z"},"execution":{"iopub.status.busy":"2024-10-04T15:55:18.997692Z","iopub.execute_input":"2024-10-04T15:55:18.998131Z","iopub.status.idle":"2024-10-04T15:55:19.008855Z","shell.execute_reply.started":"2024-10-04T15:55:18.998087Z","shell.execute_reply":"2024-10-04T15:55:19.007309Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def forward_cross_entropy_loss(probabilities, labels):\n    \"\"\"\n    Compute the output from a cross entropy loss layer given the probabilities and labels.\n\n    probabilities is of the shape (# classes)\n    labels is of the shape (# classes)\n\n    The output should be a scalar\n\n    Returns:\n        The result of the log loss layer\n    \"\"\"\n    result = 0\n\n    for i, label in enumerate(labels):\n        if label == 1:\n            result += -np.log(probabilities[i])\n\n    return result","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.610918Z","start_time":"2024-10-04T15:46:16.455008Z"},"execution":{"iopub.status.busy":"2024-10-04T15:55:23.280860Z","iopub.execute_input":"2024-10-04T15:55:23.281329Z","iopub.status.idle":"2024-10-04T15:55:23.287652Z","shell.execute_reply.started":"2024-10-04T15:55:23.281281Z","shell.execute_reply":"2024-10-04T15:55:23.286516Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def backward_cross_entropy_loss(probabilities, labels):\n    \"\"\"\n    Compute the gradient of the cross entropy loss with respect to the probabilities.\n\n    probabilities is of the shape (# classes)\n    labels is of the shape (# classes)\n\n    The output should be the gradient with respect to the probabilities.\n\n    Returns:\n        The gradient of the loss with respect to the probabilities.\n    \"\"\"\n\n    # *** START CODE HERE ***\n    # *** END CODE HERE ***\n    return - labels / probabilities","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.752836Z","start_time":"2024-10-04T15:46:16.614914Z"},"execution":{"iopub.status.busy":"2024-10-04T15:55:25.687394Z","iopub.execute_input":"2024-10-04T15:55:25.687840Z","iopub.status.idle":"2024-10-04T15:55:25.693474Z","shell.execute_reply.started":"2024-10-04T15:55:25.687796Z","shell.execute_reply":"2024-10-04T15:55:25.692287Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def forward_linear(weights, bias, data):\n    \"\"\"\n    Compute the output from a linear layer with the given weights, bias and data.\n    weights is of the shape (input # features, output # features)\n    bias is of the shape (output # features)\n    data is of the shape (input # features)\n\n    The output should be of the shape (output # features)\n\n    Returns:\n        The result of the linear layer\n    \"\"\"\n    return data.dot(weights) + bias","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:16.923571Z","start_time":"2024-10-04T15:46:16.756833Z"},"execution":{"iopub.status.busy":"2024-10-04T15:55:27.534753Z","iopub.execute_input":"2024-10-04T15:55:27.535177Z","iopub.status.idle":"2024-10-04T15:55:27.541443Z","shell.execute_reply.started":"2024-10-04T15:55:27.535136Z","shell.execute_reply":"2024-10-04T15:55:27.540281Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def backward_linear(weights, bias, data, output_grad):\n    \"\"\"\n    Compute the gradients of the loss with respect to the parameters of a linear layer.\n\n    See forward_linear for information about the shapes of the variables.\n\n    output_grad is the gradient of the loss with respect to the output of this layer.\n\n    This should return a tuple with three elements:\n    - The gradient of the loss with respect to the weights\n    - The gradient of the loss with respect to the bias\n    - The gradient of the loss with respect to the data\n    \"\"\"\n\n    # *** START CODE HERE ***\n    weights_grad = data.reshape(-1,1) @ output_grad.reshape(1,-1)\n    bias_grad = np.ones(bias.shape)\n    data_grad = weights.dot(output_grad)\n\n    return weights_grad, bias_grad, data_grad\n    # *** END CODE HERE ***","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:17.059220Z","start_time":"2024-10-04T15:46:16.926555Z"},"execution":{"iopub.status.busy":"2024-10-04T16:44:25.946310Z","iopub.execute_input":"2024-10-04T16:44:25.946746Z","iopub.status.idle":"2024-10-04T16:44:25.954910Z","shell.execute_reply.started":"2024-10-04T16:44:25.946701Z","shell.execute_reply":"2024-10-04T16:44:25.953310Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def forward_prop(data, labels, params):\n    \"\"\"\n    Implement the forward layer given the data, labels, and params.\n\n    Args:\n        data: A numpy array containing the input (shape is 1 by 28 by 28)\n        labels: A 1d numpy array containing the labels (shape is 10)\n        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n            This numpy array will contain W1, b1, W2 and b2\n            W1 and b1 represent the weights and bias for the hidden layer of the network\n            W2 and b2 represent the weights and bias for the output layer of the network\n\n    Returns:\n        A 2 element tuple containing:\n            1. A numpy array The output (after the softmax) of the output layer\n            2. The average loss for these data elements\n    \"\"\"\n\n    W1 = params['W1']\n    b1 = params['b1']\n    W2 = params['W2']\n    b2 = params['b2']\n\n    first_convolution = forward_convolution(W1, b1, data)\n    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n    first_after_relu = forward_relu(first_max_pool)\n\n    flattened = np.reshape(first_after_relu, (-1))\n\n    logits = forward_linear(W2, b2, flattened)\n\n    y = forward_softmax(logits)\n    cost = forward_cross_entropy_loss(y, labels)\n\n    return y, cost\n\ndef backward_prop(data, labels, params):\n    \"\"\"\n    Implement the backward propagation gradient computation step for a neural network\n\n    Args:\n        data: A numpy array containing the input for a single example\n        labels: A 1d numpy array containing the labels for a single example\n        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n            This numpy array will contain W1, b1, W2, and b2\n            W1 and b1 represent the weights and bias for the convolutional layer\n            W2 and b2 represent the weights and bias for the output layer of the network\n\n    Returns:\n        A dictionary of strings to numpy arrays where each key represents the name of a weight\n        and the values represent the gradient of the loss with respect to that weight.\n\n        In particular, it should have 4 elements:\n            W1, W2, b1, and b2\n    \"\"\"\n\n    # *** START CODE HERE ***\n    W1 = params['W1']\n    b1 = params['b1']\n    W2 = params['W2']\n    b2 = params['b2']\n\n    first_convolution = forward_convolution(W1, b1, data)\n    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n    first_after_relu = forward_relu(first_max_pool)\n    flattened = np.reshape(first_after_relu, (-1))\n    logits = forward_linear(W2, b2, flattened)\n    y = forward_softmax(logits)\n\n    grad_CE_loss = backward_cross_entropy_loss(\n        probabilities=y,\n        labels=labels\n    )\n    grad_softmax = backward_softmax(\n        x=logits,\n        grad_outputs=grad_CE_loss\n    )\n    grad_W2, grad_b2, grad_linear = backward_linear(\n        weights=W2,\n        bias=b2,\n        data=flattened,\n        output_grad=grad_softmax\n    )\n    grad_relu = backward_relu(\n        x=first_max_pool,\n        grad_outputs=grad_linear.reshape(first_max_pool.shape)\n    )\n    grad_max_pool = backward_max_pool(\n        data=first_convolution,\n        pool_width=MAX_POOL_SIZE,\n        pool_height=MAX_POOL_SIZE,\n        output_grad=grad_relu\n    )\n    grad_W1, grad_b1, grad_convolution = backward_convolution(\n        conv_W=W1,\n        conv_b=b1,\n        data=data,\n        output_grad=grad_max_pool\n    )\n    return {\n        'W1': grad_W1,\n        'b1': grad_b1,\n        'W2': grad_W2,\n        'b2': grad_b2\n    }\n    # *** END CODE HERE ***\n\ndef forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n\n    y_array = []\n    cost_array = []\n\n    for item, label in zip(batch_data, batch_labels):\n        y, cost = forward_prop_func(item, label, params)\n        y_array.append(y)\n        cost_array.append(cost)\n\n    return np.array(y_array), np.array(cost_array)\n\ndef gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n    \"\"\"\n    Perform one batch of gradient descent on the given training data using the provided learning rate.\n\n    This code should update the parameters stored in params.\n    It should not return anything\n\n    Args:\n        batch_data: A numpy array containing the training data for the batch\n        train_labels: A numpy array containing the training labels for the batch\n        learning_rate: The learning rate\n        params: A dict of parameter names to parameter values that should be updated.\n        backward_prop_func: A function that follows the backwards_prop API\n\n    Returns: This function returns nothing.\n    \"\"\"\n\n    total_grad = {}\n\n    for i in range(batch_data.shape[0]):\n        grad = backward_prop_func(\n            batch_data[i, :, :],\n            batch_labels[i, :],\n            params)\n        for key, value in grad.items():\n            if key not in total_grad:\n                total_grad[key] = np.zeros(value.shape)\n\n            total_grad[key] += value\n\n    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n\n    # This function does not return anything\n    return\n\ndef nn_train(\n    train_data, train_labels, dev_data, dev_labels,\n    get_initial_params_func, forward_prop_func, backward_prop_func,\n    learning_rate=5.0, batch_size=16, num_batches=400):\n\n    m = train_data.shape[0]\n\n    params = get_initial_params_func()\n\n    cost_dev = []\n    accuracy_dev = []\n    for batch in range(num_batches):\n        print('Currently processing {} / {}'.format(batch, num_batches))\n\n        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n\n        if batch % 100 == 0:\n            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n            cost_dev.append(sum(cost) / len(cost))\n            accuracy_dev.append(compute_accuracy(output, dev_labels))\n\n            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n\n        gradient_descent_batch(batch_data, batch_labels,\n            learning_rate, params, backward_prop_func)\n\n    return params, cost_dev, accuracy_dev\n\ndef nn_test(data, labels, params):\n    output, cost = forward_prop(data, labels, params)\n    accuracy = compute_accuracy(output, labels)\n    return accuracy\n\ndef compute_accuracy(output, labels):\n    correct_output = np.argmax(output,axis=1)\n    correct_labels = np.argmax(labels,axis=1)\n\n    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n\n    accuracy = sum(is_correct) * 1. / labels.shape[0]\n    return accuracy\n\ndef one_hot_labels(labels):\n    one_hot_labels = np.zeros((labels.size, 10))\n    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n    return one_hot_labels\n\ndef read_data(images_file, labels_file):\n    x = np.loadtxt(images_file, delimiter=',')\n    y = np.loadtxt(labels_file, delimiter=',')\n\n    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n\n    return x, y\n\ndef run_train(all_data, all_labels, backward_prop_func):\n    params, cost_dev, accuracy_dev = nn_train(\n        all_data['train'], all_labels['train'],\n        all_data['dev'], all_labels['dev'],\n        get_initial_params, forward_prop, backward_prop_func,\n        learning_rate=1e-2, batch_size=16, num_batches=400\n    )\n\n    t = np.arange(400 // 100)\n\n    fig, (ax1, ax2) = plt.subplots(2, 1)\n\n    ax1.plot(t, cost_dev, 'b')\n    ax1.set_xlabel('time')\n    ax1.set_ylabel('loss')\n    ax1.set_title('Training curve')\n\n    ax2.plot(t, accuracy_dev, 'b')\n    ax2.set_xlabel('time')\n    ax2.set_ylabel('accuracy')\n\n    fig.savefig('output/train.png')\n\ndef main():\n    np.random.seed(100)\n    train_data, train_labels = read_data('/kaggle/input/convolutionmnistdataset/images_train.csv', \n                                         '/kaggle/input/convolutionmnistdataset/labels_train.csv')\n    train_labels = one_hot_labels(train_labels)\n    p = np.random.permutation(60000)\n    train_data = train_data[p,:]\n    train_labels = train_labels[p,:]\n\n    dev_data = train_data[0:400,:]\n    dev_labels = train_labels[0:400,:]\n    train_data = train_data[400:,:]\n    train_labels = train_labels[400:,:]\n\n    mean = np.mean(train_data)\n    std = np.std(train_data)\n    train_data = (train_data - mean) / std\n    dev_data = (dev_data - mean) / std\n\n    all_data = {\n        'train': train_data,\n        'dev': dev_data,\n    }\n\n    all_labels = {\n        'train': train_labels,\n        'dev': dev_labels,\n    }\n\n    run_train(all_data, all_labels, backward_prop)","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:17.178487Z","start_time":"2024-10-04T15:46:17.063209Z"},"execution":{"iopub.status.busy":"2024-10-04T16:45:16.231688Z","iopub.execute_input":"2024-10-04T16:45:16.232107Z","iopub.status.idle":"2024-10-04T16:45:16.354510Z","shell.execute_reply.started":"2024-10-04T16:45:16.232063Z","shell.execute_reply":"2024-10-04T16:45:16.353296Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"ExecuteTime":{"end_time":"2024-10-04T15:46:22.859746Z","start_time":"2024-10-04T15:46:17.182478Z"},"execution":{"iopub.status.busy":"2024-10-04T17:33:30.959274Z","iopub.execute_input":"2024-10-04T17:33:30.960009Z","iopub.status.idle":"2024-10-04T17:37:46.562537Z","shell.execute_reply.started":"2024-10-04T17:33:30.959966Z","shell.execute_reply":"2024-10-04T17:37:46.561311Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Currently processing 0 / 400\nCost and accuracy 2.721417647426753 0.0725\nCurrently processing 1 / 400\nCurrently processing 2 / 400\nCurrently processing 3 / 400\nCurrently processing 4 / 400\nCurrently processing 5 / 400\nCurrently processing 6 / 400\nCurrently processing 7 / 400\nCurrently processing 8 / 400\nCurrently processing 9 / 400\nCurrently processing 10 / 400\nCurrently processing 11 / 400\nCurrently processing 12 / 400\nCurrently processing 13 / 400\nCurrently processing 14 / 400\nCurrently processing 15 / 400\nCurrently processing 16 / 400\nCurrently processing 17 / 400\nCurrently processing 18 / 400\nCurrently processing 19 / 400\nCurrently processing 20 / 400\nCurrently processing 21 / 400\nCurrently processing 22 / 400\nCurrently processing 23 / 400\nCurrently processing 24 / 400\nCurrently processing 25 / 400\nCurrently processing 26 / 400\nCurrently processing 27 / 400\nCurrently processing 28 / 400\nCurrently processing 29 / 400\nCurrently processing 30 / 400\nCurrently processing 31 / 400\nCurrently processing 32 / 400\nCurrently processing 33 / 400\nCurrently processing 34 / 400\nCurrently processing 35 / 400\nCurrently processing 36 / 400\nCurrently processing 37 / 400\nCurrently processing 38 / 400\nCurrently processing 39 / 400\nCurrently processing 40 / 400\nCurrently processing 41 / 400\nCurrently processing 42 / 400\nCurrently processing 43 / 400\nCurrently processing 44 / 400\nCurrently processing 45 / 400\nCurrently processing 46 / 400\nCurrently processing 47 / 400\nCurrently processing 48 / 400\nCurrently processing 49 / 400\nCurrently processing 50 / 400\nCurrently processing 51 / 400\nCurrently processing 52 / 400\nCurrently processing 53 / 400\nCurrently processing 54 / 400\nCurrently processing 55 / 400\nCurrently processing 56 / 400\nCurrently processing 57 / 400\nCurrently processing 58 / 400\nCurrently processing 59 / 400\nCurrently processing 60 / 400\nCurrently processing 61 / 400\nCurrently processing 62 / 400\nCurrently processing 63 / 400\nCurrently processing 64 / 400\nCurrently processing 65 / 400\nCurrently processing 66 / 400\nCurrently processing 67 / 400\nCurrently processing 68 / 400\nCurrently processing 69 / 400\nCurrently processing 70 / 400\nCurrently processing 71 / 400\nCurrently processing 72 / 400\nCurrently processing 73 / 400\nCurrently processing 74 / 400\nCurrently processing 75 / 400\nCurrently processing 76 / 400\nCurrently processing 77 / 400\nCurrently processing 78 / 400\nCurrently processing 79 / 400\nCurrently processing 80 / 400\nCurrently processing 81 / 400\nCurrently processing 82 / 400\nCurrently processing 83 / 400\nCurrently processing 84 / 400\nCurrently processing 85 / 400\nCurrently processing 86 / 400\nCurrently processing 87 / 400\nCurrently processing 88 / 400\nCurrently processing 89 / 400\nCurrently processing 90 / 400\nCurrently processing 91 / 400\nCurrently processing 92 / 400\nCurrently processing 93 / 400\nCurrently processing 94 / 400\nCurrently processing 95 / 400\nCurrently processing 96 / 400\nCurrently processing 97 / 400\nCurrently processing 98 / 400\nCurrently processing 99 / 400\nCurrently processing 100 / 400\nCost and accuracy 1.1099922592765652 0.7125\nCurrently processing 101 / 400\nCurrently processing 102 / 400\nCurrently processing 103 / 400\nCurrently processing 104 / 400\nCurrently processing 105 / 400\nCurrently processing 106 / 400\nCurrently processing 107 / 400\nCurrently processing 108 / 400\nCurrently processing 109 / 400\nCurrently processing 110 / 400\nCurrently processing 111 / 400\nCurrently processing 112 / 400\nCurrently processing 113 / 400\nCurrently processing 114 / 400\nCurrently processing 115 / 400\nCurrently processing 116 / 400\nCurrently processing 117 / 400\nCurrently processing 118 / 400\nCurrently processing 119 / 400\nCurrently processing 120 / 400\nCurrently processing 121 / 400\nCurrently processing 122 / 400\nCurrently processing 123 / 400\nCurrently processing 124 / 400\nCurrently processing 125 / 400\nCurrently processing 126 / 400\nCurrently processing 127 / 400\nCurrently processing 128 / 400\nCurrently processing 129 / 400\nCurrently processing 130 / 400\nCurrently processing 131 / 400\nCurrently processing 132 / 400\nCurrently processing 133 / 400\nCurrently processing 134 / 400\nCurrently processing 135 / 400\nCurrently processing 136 / 400\nCurrently processing 137 / 400\nCurrently processing 138 / 400\nCurrently processing 139 / 400\nCurrently processing 140 / 400\nCurrently processing 141 / 400\nCurrently processing 142 / 400\nCurrently processing 143 / 400\nCurrently processing 144 / 400\nCurrently processing 145 / 400\nCurrently processing 146 / 400\nCurrently processing 147 / 400\nCurrently processing 148 / 400\nCurrently processing 149 / 400\nCurrently processing 150 / 400\nCurrently processing 151 / 400\nCurrently processing 152 / 400\nCurrently processing 153 / 400\nCurrently processing 154 / 400\nCurrently processing 155 / 400\nCurrently processing 156 / 400\nCurrently processing 157 / 400\nCurrently processing 158 / 400\nCurrently processing 159 / 400\nCurrently processing 160 / 400\nCurrently processing 161 / 400\nCurrently processing 162 / 400\nCurrently processing 163 / 400\nCurrently processing 164 / 400\nCurrently processing 165 / 400\nCurrently processing 166 / 400\nCurrently processing 167 / 400\nCurrently processing 168 / 400\nCurrently processing 169 / 400\nCurrently processing 170 / 400\nCurrently processing 171 / 400\nCurrently processing 172 / 400\nCurrently processing 173 / 400\nCurrently processing 174 / 400\nCurrently processing 175 / 400\nCurrently processing 176 / 400\nCurrently processing 177 / 400\nCurrently processing 178 / 400\nCurrently processing 179 / 400\nCurrently processing 180 / 400\nCurrently processing 181 / 400\nCurrently processing 182 / 400\nCurrently processing 183 / 400\nCurrently processing 184 / 400\nCurrently processing 185 / 400\nCurrently processing 186 / 400\nCurrently processing 187 / 400\nCurrently processing 188 / 400\nCurrently processing 189 / 400\nCurrently processing 190 / 400\nCurrently processing 191 / 400\nCurrently processing 192 / 400\nCurrently processing 193 / 400\nCurrently processing 194 / 400\nCurrently processing 195 / 400\nCurrently processing 196 / 400\nCurrently processing 197 / 400\nCurrently processing 198 / 400\nCurrently processing 199 / 400\nCurrently processing 200 / 400\nCost and accuracy 0.4886883045083733 0.8475\nCurrently processing 201 / 400\nCurrently processing 202 / 400\nCurrently processing 203 / 400\nCurrently processing 204 / 400\nCurrently processing 205 / 400\nCurrently processing 206 / 400\nCurrently processing 207 / 400\nCurrently processing 208 / 400\nCurrently processing 209 / 400\nCurrently processing 210 / 400\nCurrently processing 211 / 400\nCurrently processing 212 / 400\nCurrently processing 213 / 400\nCurrently processing 214 / 400\nCurrently processing 215 / 400\nCurrently processing 216 / 400\nCurrently processing 217 / 400\nCurrently processing 218 / 400\nCurrently processing 219 / 400\nCurrently processing 220 / 400\nCurrently processing 221 / 400\nCurrently processing 222 / 400\nCurrently processing 223 / 400\nCurrently processing 224 / 400\nCurrently processing 225 / 400\nCurrently processing 226 / 400\nCurrently processing 227 / 400\nCurrently processing 228 / 400\nCurrently processing 229 / 400\nCurrently processing 230 / 400\nCurrently processing 231 / 400\nCurrently processing 232 / 400\nCurrently processing 233 / 400\nCurrently processing 234 / 400\nCurrently processing 235 / 400\nCurrently processing 236 / 400\nCurrently processing 237 / 400\nCurrently processing 238 / 400\nCurrently processing 239 / 400\nCurrently processing 240 / 400\nCurrently processing 241 / 400\nCurrently processing 242 / 400\nCurrently processing 243 / 400\nCurrently processing 244 / 400\nCurrently processing 245 / 400\nCurrently processing 246 / 400\nCurrently processing 247 / 400\nCurrently processing 248 / 400\nCurrently processing 249 / 400\nCurrently processing 250 / 400\nCurrently processing 251 / 400\nCurrently processing 252 / 400\nCurrently processing 253 / 400\nCurrently processing 254 / 400\nCurrently processing 255 / 400\nCurrently processing 256 / 400\nCurrently processing 257 / 400\nCurrently processing 258 / 400\nCurrently processing 259 / 400\nCurrently processing 260 / 400\nCurrently processing 261 / 400\nCurrently processing 262 / 400\nCurrently processing 263 / 400\nCurrently processing 264 / 400\nCurrently processing 265 / 400\nCurrently processing 266 / 400\nCurrently processing 267 / 400\nCurrently processing 268 / 400\nCurrently processing 269 / 400\nCurrently processing 270 / 400\nCurrently processing 271 / 400\nCurrently processing 272 / 400\nCurrently processing 273 / 400\nCurrently processing 274 / 400\nCurrently processing 275 / 400\nCurrently processing 276 / 400\nCurrently processing 277 / 400\nCurrently processing 278 / 400\nCurrently processing 279 / 400\nCurrently processing 280 / 400\nCurrently processing 281 / 400\nCurrently processing 282 / 400\nCurrently processing 283 / 400\nCurrently processing 284 / 400\nCurrently processing 285 / 400\nCurrently processing 286 / 400\nCurrently processing 287 / 400\nCurrently processing 288 / 400\nCurrently processing 289 / 400\nCurrently processing 290 / 400\nCurrently processing 291 / 400\nCurrently processing 292 / 400\nCurrently processing 293 / 400\nCurrently processing 294 / 400\nCurrently processing 295 / 400\nCurrently processing 296 / 400\nCurrently processing 297 / 400\nCurrently processing 298 / 400\nCurrently processing 299 / 400\nCurrently processing 300 / 400\nCost and accuracy 0.4955593193759681 0.8675\nCurrently processing 301 / 400\nCurrently processing 302 / 400\nCurrently processing 303 / 400\nCurrently processing 304 / 400\nCurrently processing 305 / 400\nCurrently processing 306 / 400\nCurrently processing 307 / 400\nCurrently processing 308 / 400\nCurrently processing 309 / 400\nCurrently processing 310 / 400\nCurrently processing 311 / 400\nCurrently processing 312 / 400\nCurrently processing 313 / 400\nCurrently processing 314 / 400\nCurrently processing 315 / 400\nCurrently processing 316 / 400\nCurrently processing 317 / 400\nCurrently processing 318 / 400\nCurrently processing 319 / 400\nCurrently processing 320 / 400\nCurrently processing 321 / 400\nCurrently processing 322 / 400\nCurrently processing 323 / 400\nCurrently processing 324 / 400\nCurrently processing 325 / 400\nCurrently processing 326 / 400\nCurrently processing 327 / 400\nCurrently processing 328 / 400\nCurrently processing 329 / 400\nCurrently processing 330 / 400\nCurrently processing 331 / 400\nCurrently processing 332 / 400\nCurrently processing 333 / 400\nCurrently processing 334 / 400\nCurrently processing 335 / 400\nCurrently processing 336 / 400\nCurrently processing 337 / 400\nCurrently processing 338 / 400\nCurrently processing 339 / 400\nCurrently processing 340 / 400\nCurrently processing 341 / 400\nCurrently processing 342 / 400\nCurrently processing 343 / 400\nCurrently processing 344 / 400\nCurrently processing 345 / 400\nCurrently processing 346 / 400\nCurrently processing 347 / 400\nCurrently processing 348 / 400\nCurrently processing 349 / 400\nCurrently processing 350 / 400\nCurrently processing 351 / 400\nCurrently processing 352 / 400\nCurrently processing 353 / 400\nCurrently processing 354 / 400\nCurrently processing 355 / 400\nCurrently processing 356 / 400\nCurrently processing 357 / 400\nCurrently processing 358 / 400\nCurrently processing 359 / 400\nCurrently processing 360 / 400\nCurrently processing 361 / 400\nCurrently processing 362 / 400\nCurrently processing 363 / 400\nCurrently processing 364 / 400\nCurrently processing 365 / 400\nCurrently processing 366 / 400\nCurrently processing 367 / 400\nCurrently processing 368 / 400\nCurrently processing 369 / 400\nCurrently processing 370 / 400\nCurrently processing 371 / 400\nCurrently processing 372 / 400\nCurrently processing 373 / 400\nCurrently processing 374 / 400\nCurrently processing 375 / 400\nCurrently processing 376 / 400\nCurrently processing 377 / 400\nCurrently processing 378 / 400\nCurrently processing 379 / 400\nCurrently processing 380 / 400\nCurrently processing 381 / 400\nCurrently processing 382 / 400\nCurrently processing 383 / 400\nCurrently processing 384 / 400\nCurrently processing 385 / 400\nCurrently processing 386 / 400\nCurrently processing 387 / 400\nCurrently processing 388 / 400\nCurrently processing 389 / 400\nCurrently processing 390 / 400\nCurrently processing 391 / 400\nCurrently processing 392 / 400\nCurrently processing 393 / 400\nCurrently processing 394 / 400\nCurrently processing 395 / 400\nCurrently processing 396 / 400\nCurrently processing 397 / 400\nCurrently processing 398 / 400\nCurrently processing 399 / 400\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}}]}]}