{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from env import CartPole, Physics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "\"\"\"\n",
    "Parts of the code (cart and pole dynamics, and the state\n",
    "discretization) are inspired from code available at the RL repository\n",
    "http://www-anw.cs.umass.edu/rlr/domains.html\n",
    "\n",
    "Briefly, the cart-pole system is described in `p06_cartpole.py`. The main\n",
    "simulation loop in this file calls the `simulate()` function for\n",
    "simulating the pole dynamics, `get_state()` for discretizing the\n",
    "otherwise continuous state space in discrete states, and `show_cart()`\n",
    "for display.\n",
    "\n",
    "Some useful parameters are listed below:\n",
    "\n",
    "`NUM_STATES`: Number of states in the discretized state space\n",
    "You must assume that states are numbered 0 through `NUM_STATES` - 1. The\n",
    "state numbered `NUM_STATES` - 1 (the last one) is a special state that\n",
    "marks the state when the pole has been judged to have fallen (or when\n",
    "the cart is out of bounds). However, you should NOT treat this state\n",
    "any differently in your code. Any distinctions you need to make between\n",
    "states should come automatically from your learning algorithm.\n",
    "\n",
    "After each simulation cycle, you are supposed to update the transition\n",
    "counts and rewards observed. However, you should not change either\n",
    "your value function or the transition probability matrix at each\n",
    "cycle.\n",
    "\n",
    "Whenever the pole falls, a section of your code below will be\n",
    "executed. At this point, you must use the transition counts and reward\n",
    "observations that you have gathered to generate a new model for the MDP\n",
    "(i.e. transition probabilities and state rewards). After that, you\n",
    "must use value iteration to get the optimal value function for this MDP\n",
    "model.\n",
    "\n",
    "`TOLERANCE`: Controls the convergence criteria for each value iteration\n",
    "run. In value iteration, you can assume convergence when the maximum\n",
    "absolute change in the value function at any state in an iteration\n",
    "becomes lower than `TOLERANCE.\n",
    "\n",
    "You need to write code that chooses the best action according\n",
    "to your current value function, and the current model of the MDP. The\n",
    "action must be either 0 or 1 (corresponding to possible directions of\n",
    "pushing the cart)\n",
    "\n",
    "Finally, we assume that the simulation has converged when\n",
    "`NO_LEARNING_THRESHOLD` consecutive value function computations all\n",
    "converged within one value function iteration. Intuitively, it seems\n",
    "like there will be little learning after this, so we end the simulation\n",
    "here, and say the overall algorithm has converged.\n",
    "\n",
    "\n",
    "Learning curves can be generated by calling a code snippet at the end\n",
    "(it assumes that the learning was just executed, and the array\n",
    "`time_steps_to_failure` that records the time for which the pole was\n",
    "balanced before each failure is in memory). `num_failures` is a variable\n",
    "that stores the number of failures (pole drops / cart out of bounds)\n",
    "till now.\n",
    "\n",
    "Other parameters in the code are described below:\n",
    "\n",
    "`GAMMA`: Discount factor to be used\n",
    "\n",
    "The following parameters control the simulation display; you dont\n",
    "really need to know about them:\n",
    "\n",
    "`pause_time`: Controls the pause between successive frames of the\n",
    "display. Higher values make your simulation slower.\n",
    "`min_trial_length_to_start_display`: Allows you to start the display only\n",
    "after the pole has been successfully balanced for at least this many\n",
    "trials. Setting this to zero starts the display immediately. Choosing a\n",
    "reasonably high value (around 100) can allow you to rush through the\n",
    "initial learning quickly, and start the display only after the\n",
    "performance is reasonable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mdp_data(num_states):\n",
    "    \"\"\"\n",
    "    Return a variable that contains all the parameters/state you need for your MDP.\n",
    "    Feel free to use whatever data type is most convenient for you (custom classes, tuples, dicts, etc)\n",
    "\n",
    "    Assume that no transitions or rewards have been observed.\n",
    "    Initialize the value function array to small random values (0 to 0.10, say).\n",
    "    Initialize the transition probabilities uniformly (ie, probability of\n",
    "        transitioning for state x to state y using action a is exactly\n",
    "        1/num_states).\n",
    "    Initialize all state rewards to zero.\n",
    "\n",
    "    Args:\n",
    "        num_states: The number of states\n",
    "\n",
    "    Returns: The initial MDP parameters\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((num_states, num_states, 2))\n",
    "    transition_probs = np.ones((num_states, num_states, 2)) / num_states\n",
    "    # Index zero is count of rewards being -1 , index 1 is count of total num state is reached\n",
    "    reward_counts = np.zeros((num_states, 2))\n",
    "    reward = np.zeros(num_states)\n",
    "    value = np.random.rand(num_states) * 0.1\n",
    "\n",
    "    return {\n",
    "        'transition_counts': transition_counts,\n",
    "        'transition_probs': transition_probs,\n",
    "        'reward_counts': reward_counts,\n",
    "        'reward': reward,\n",
    "        'value': value,\n",
    "        'num_states': num_states,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb959483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, mdp_data):\n",
    "    \"\"\"\n",
    "    Choose the next action (0 or 1) that is optimal according to your current\n",
    "    mdp_data. When there is no optimal action, return a random action.\n",
    "\n",
    "    Args:\n",
    "        state: The current state in the MDP\n",
    "        mdp_data: The parameters for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        0 or 1 that is optimal according to your current MDP\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    expect_value = mdp_data['value'].dot(mdp_data['transition_probs'][state])\n",
    "    if expect_value[0] == expect_value[1]:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        return np.argmax(expect_value)\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, reward):\n",
    "    \"\"\"\n",
    "    Update the transition count and reward count information in your mdp_data.\n",
    "    Do not change the other MDP parameters (those get changed later).\n",
    "\n",
    "    Record the number of times `state, action, new_state` occurs.\n",
    "    Record the rewards for every `new_state`\n",
    "    (since rewards are -1 or 0, you just need to record number of times reward -1 is seen in 'reward_counts' index new_state,0)\n",
    "    Record the number of time `new_state` was reached (in 'reward_counts' index new_state,1)\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The parameters of your MDP. See initialize_mdp_data.\n",
    "        state: The state that was observed at the start.\n",
    "        action: The action you performed.\n",
    "        new_state: The state after your action.\n",
    "        reward: The reward after your action (i.e. reward corresponding to new_state).\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    mdp_data['transition_counts'][state, new_state, action] += 1\n",
    "\n",
    "    if reward == -1:\n",
    "        mdp_data['reward_counts'][new_state, 0] += 1\n",
    "\n",
    "    mdp_data['reward_counts'][new_state, 1] += 1\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab16ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mdp_transition_probs_reward(mdp_data):\n",
    "    \"\"\"\n",
    "    Update the estimated transition probabilities and reward values in your MDP.\n",
    "\n",
    "    Make sure you account for the case when a state-action pair has never\n",
    "    been tried before, or the state has never been visited before. In that\n",
    "    case, you must not change that component (and thus keep it at the\n",
    "    initialized uniform distribution).\n",
    "    \n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    transition_counts = mdp_data['transition_counts']\n",
    "    num_counts = transition_counts.sum(axis=1)\n",
    "    num_states = transition_counts.shape[0]\n",
    "    for i in range(num_states):\n",
    "        for a in range(2):\n",
    "            if  num_counts[i, a]  != 0:\n",
    "                mdp_data['transition_probs'][i, :, a] = transition_counts[i, :, a] / num_counts[i, a]\n",
    "\n",
    "    reward_counts = mdp_data['reward_counts']\n",
    "    for k in range(num_states):\n",
    "        sum_count = reward_counts[k, 1]\n",
    "        if sum_count != 0:\n",
    "            mdp_data['reward'][k] = -reward_counts[k, 0] / sum_count\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49219d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mdp_value(mdp_data, tolerance, gamma):\n",
    "    \"\"\"\n",
    "    Update the estimated values in your MDP.\n",
    "\n",
    "    Perform value iteration using the new estimated model for the MDP.\n",
    "    The convergence criterion should be based on `TOLERANCE` as described\n",
    "    at the top of the file.\n",
    "\n",
    "    Return true if it converges within one iteration.\n",
    "    \n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "        tolerance: The tolerance to use for the convergence criterion.\n",
    "        gamma: Your discount factor.\n",
    "\n",
    "    Returns:\n",
    "        True if the value iteration converged in one iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    iters = 0\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "\n",
    "        value = mdp_data['value']\n",
    "        new_value = mdp_data['reward'] + gamma * value.dot(transition_probs).max(axis=1)\n",
    "        mdp_data['value'] = new_value\n",
    "\n",
    "        if np.max(np.abs(value - new_value)) < tolerance:\n",
    "            break\n",
    "\n",
    "    return iters==1\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(plot=True):\n",
    "    # Seed the randomness of the simulation so this outputs the same thing each time\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Simulation parameters\n",
    "    pause_time = 0.0001\n",
    "    min_trial_length_to_start_display = 100\n",
    "    display_started = min_trial_length_to_start_display == 0\n",
    "\n",
    "    NUM_STATES = 163\n",
    "    GAMMA = 0.995\n",
    "    TOLERANCE = 0.01\n",
    "    NO_LEARNING_THRESHOLD = 20\n",
    "\n",
    "    # Time cycle of the simulation\n",
    "    time = 0\n",
    "\n",
    "    # These variables perform bookkeeping (how many cycles was the pole\n",
    "    # balanced for before it fell). Useful for plotting learning curves.\n",
    "    time_steps_to_failure = []\n",
    "    num_failures = 0\n",
    "    time_at_start_of_current_trial = 0\n",
    "\n",
    "    # You should reach convergence well before this\n",
    "    max_failures = 500\n",
    "\n",
    "    # Initialize a cart pole\n",
    "    cart_pole = CartPole(Physics())\n",
    "\n",
    "    # Starting `state_tuple` is (0, 0, 0, 0)\n",
    "    # x, x_dot, theta, theta_dot represents the actual continuous state vector\n",
    "    x, x_dot, theta, theta_dot = 0.0, 0.0, 0.0, 0.0\n",
    "    state_tuple = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    # `state` is the number given to this state, you only need to consider\n",
    "    # this representation of the state\n",
    "    state = cart_pole.get_state(state_tuple)\n",
    "    # if min_trial_length_to_start_display == 0 or display_started == 1:\n",
    "    #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "    mdp_data = initialize_mdp_data(NUM_STATES)\n",
    "\n",
    "    # This is the criterion to end the simulation.\n",
    "    # You should change it to terminate when the previous\n",
    "    # 'NO_LEARNING_THRESHOLD' consecutive value function computations all\n",
    "    # converged within one value function iteration. Intuitively, it seems\n",
    "    # like there will be little learning after this, so end the simulation\n",
    "    # here, and say the overall algorithm has converged.\n",
    "\n",
    "    consecutive_no_learning_trials = 0\n",
    "    while consecutive_no_learning_trials < NO_LEARNING_THRESHOLD:\n",
    "\n",
    "        action = choose_action(state, mdp_data)\n",
    "\n",
    "        # Get the next state by simulating the dynamics\n",
    "        state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "        # x, x_dot, theta, theta_dot = state_tuple\n",
    "\n",
    "        # Increment simulation time\n",
    "        time = time + 1\n",
    "\n",
    "        # Get the state number corresponding to new state vector\n",
    "        new_state = cart_pole.get_state(state_tuple)\n",
    "        # if display_started == 1:\n",
    "        #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "        # reward function to use - do not change this!\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            R = -1\n",
    "        else:\n",
    "            R = 0\n",
    "\n",
    "        update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, R)\n",
    "\n",
    "        # Recompute MDP model whenever pole falls\n",
    "        # Compute the value function V for the new model\n",
    "        if new_state == NUM_STATES - 1:\n",
    "\n",
    "            update_mdp_transition_probs_reward(mdp_data)\n",
    "\n",
    "            converged_in_one_iteration = update_mdp_value(mdp_data, TOLERANCE, GAMMA)\n",
    "\n",
    "            if converged_in_one_iteration:\n",
    "                consecutive_no_learning_trials = consecutive_no_learning_trials + 1\n",
    "            else:\n",
    "                consecutive_no_learning_trials = 0\n",
    "\n",
    "        # Do NOT change this code: Controls the simulation, and handles the case\n",
    "        # when the pole fell and the state must be reinitialized.\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            num_failures += 1\n",
    "            if num_failures >= max_failures:\n",
    "                break\n",
    "            print('[INFO] Failure number {}'.format(num_failures))\n",
    "            time_steps_to_failure.append(time - time_at_start_of_current_trial)\n",
    "            # time_steps_to_failure[num_failures] = time - time_at_start_of_current_trial\n",
    "            time_at_start_of_current_trial = time\n",
    "\n",
    "            if time_steps_to_failure[num_failures - 1] > min_trial_length_to_start_display:\n",
    "                display_started = 1\n",
    "\n",
    "            # Reinitialize state\n",
    "            # x = 0.0\n",
    "            x = -1.1 + np.random.uniform() * 2.2\n",
    "            x_dot, theta, theta_dot = 0.0, 0.0, 0.0\n",
    "            state_tuple = (x, x_dot, theta, theta_dot)\n",
    "            state = cart_pole.get_state(state_tuple)\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "    if plot:\n",
    "        # plot the learning curve (time balanced vs. trial)\n",
    "        log_tstf = np.log(np.array(time_steps_to_failure))\n",
    "        plt.plot(np.arange(len(time_steps_to_failure)), log_tstf, 'k')\n",
    "        window = 30\n",
    "        w = np.array([1/window for _ in range(window)])\n",
    "        weights = lfilter(w, 1, log_tstf)\n",
    "        x = np.arange(window//2, len(log_tstf) - window//2)\n",
    "        plt.plot(x, weights[window:len(log_tstf)], 'r--')\n",
    "        plt.xlabel('Num failures')\n",
    "        plt.ylabel('Log of num steps to failure')\n",
    "        plt.title('seed = {}'.format(seed))\n",
    "        plt.savefig('output/control_{}.png'.format(seed))\n",
    "\n",
    "    return np.array(time_steps_to_failure)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
